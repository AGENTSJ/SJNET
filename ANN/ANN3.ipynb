{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset=[10,11]\n",
    "learningRate=0.0002\n",
    "errorThresh=1\n",
    "\n",
    "class Layer:\n",
    "    \n",
    "    def __init__(self,neuronCount=None,position=None,neuronvals=None):\n",
    "        \n",
    "        self.neuronCount = neuronCount\n",
    "        self.position = position\n",
    "        self.neuronvals = []\n",
    "        self.weights = []\n",
    "        self.bias = []\n",
    "        self.NodeDeltas=[] \n",
    "\n",
    "        #set by setintialweights and setnodedelta\n",
    "        self.previousLayer =None\n",
    "        self.AfterLayer =None\n",
    "\n",
    "        for i in range(self.neuronCount):\n",
    "\n",
    "            # self.bias.append(random.uniform(0.0001,1))\n",
    "            self.bias.append(1)\n",
    "\n",
    "        if position==1:\n",
    "            self.neuronvals = neuronvals\n",
    "    \n",
    "    def setInitialWeights(self,previousLayer):\n",
    "\n",
    "        self.previousLayer = previousLayer\n",
    "        test =[\n",
    "            [1,2,3,4]\n",
    "        ]\n",
    "        for i in range(self.neuronCount):\n",
    "            temp = []\n",
    "            for j in range(previousLayer.neuronCount):\n",
    "                temp.append(random.uniform(0.0001,1))\n",
    "                # temp.append(test[0][i+j])\n",
    "\n",
    "            self.weights.append(temp)\n",
    "\n",
    "    def setNodeDelta(self,AfterLayer):\n",
    "        \"\"\"\"\"\n",
    "        set nodedelta value for layers\n",
    "        \"\"\"\"\"\n",
    "        self.AfterLayer = AfterLayer\n",
    "        if self.position == -1:\n",
    "            # outputlayer\n",
    "            if len(self.NodeDeltas)==0:\n",
    "                for i in range(self.neuronCount):\n",
    "\n",
    "                    nodedelta = self.neuronvals[i]-dataset[i] #exepectd to change dimenston CHNGDIMN\n",
    "                    self.NodeDeltas.append(nodedelta)\n",
    "            else:\n",
    "                for i in range(self.neuronCount):\n",
    "\n",
    "                    nodedelta = self.neuronvals[i]-dataset[i] #exepectd to change dimenston CHNGDIMN\n",
    "                    self.NodeDeltas[i]=nodedelta\n",
    "        else:\n",
    "            # hidden layers\n",
    "            if len(self.NodeDeltas)==0:\n",
    "\n",
    "                for i in range(self.neuronCount):\n",
    "                    nodedelta= 0\n",
    "                    for j in range(AfterLayer.neuronCount):\n",
    "                        nodedelta = nodedelta+AfterLayer.NodeDeltas[j]*self.weights[i][j]\n",
    "                    self.NodeDeltas.append(nodedelta)\n",
    "\n",
    "            else:\n",
    "                    for i in range(self.neuronCount):\n",
    "                        nodedelta= 0\n",
    "                        for j in range(AfterLayer.neuronCount):\n",
    "                            nodedelta = nodedelta+AfterLayer.NodeDeltas[j]*self.weights[i][j]\n",
    "                        self.NodeDeltas[i]=nodedelta\n",
    "    \n",
    "    def updateWeightsandBias(self):\n",
    "\n",
    "        len_weight = len(self.weights[0])# based on previous layer of no(neurons) lenof weights is set\n",
    "\n",
    "        for i in range(self.neuronCount):\n",
    "\n",
    "            for j in range(len_weight):\n",
    "                \n",
    "                new_Weight = self.weights[i][j] - learningRate * self.NodeDeltas[i] * self.previousLayer.neuronvals[j]\n",
    "                self.weights[i][j] = new_Weight\n",
    "            \n",
    "            new_Bias = self.bias[i] - learningRate * self.NodeDeltas[i]\n",
    "            self.bias[i]=new_Bias        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Ann:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.LayerArr = []\n",
    "        self.batchSize = -1\n",
    "\n",
    "    def strap(self,layer1=None,layer2=None):\n",
    "        \"sets the initial weights of the ANN\"\n",
    "        layer2.setInitialWeights(layer1)\n",
    "\n",
    "    def forward(self,layer1=None,layer2=None):\n",
    "        \"performs forward passing for all layers in network\"  \n",
    "        if len(layer2.neuronvals)==0:\n",
    "            for i in range(layer2.neuronCount):\n",
    "                layer2.neuronvals.append(np.dot(layer1.neuronvals,layer2.weights[i])+layer2.bias[i])\n",
    "        else:\n",
    "            for i in range(layer2.neuronCount):\n",
    "                layer2.neuronvals[i]=np.dot(layer1.neuronvals,layer2.weights[i]+layer2.bias[i])\n",
    "            \n",
    "    def backPropagate(self,currentLayer=None,AfterLayer=None):\n",
    "        \"\"\"\"\"\n",
    "        performs back propagation by\n",
    "\n",
    "        1. seting nodeDelta (a function in lasyer class)\n",
    "        2.Update werights and biases (a function in layer class)\n",
    "        \n",
    "        \"\"\"\"\"\n",
    "        \n",
    "        \n",
    "        \n",
    "        currentLayer.setNodeDelta(AfterLayer)\n",
    "        currentLayer.updateWeightsandBias()\n",
    "        \n",
    "    def compile(self,batchSize=-1):\n",
    "        \"\"\"\"\"\n",
    "        calls strap function for every layer : initilizes weights in Ann\n",
    "        \"\"\"\"\"\n",
    "        self.batchSize = batchSize\n",
    "        for i in range(len(self.LayerArr)-1):\n",
    "            self.strap(layer1=self.LayerArr[i],layer2=self.LayerArr[i+1])\n",
    "    \n",
    "    def findError(self):\n",
    "        error = 0\n",
    "        for i in range(len(self.LayerArr[-1].neuronvals)):#CHNGDIMN\n",
    "            error = error+(dataset[i]-self.LayerArr[-1].neuronvals[i])**2\n",
    "        error = 0.5*error\n",
    "        return error\n",
    "    \n",
    "    def Train(self):\n",
    "        \"\"\"\n",
    "        Trains the model \n",
    "        \n",
    "        \"\"\"\n",
    "        errorrate = 9999999 \n",
    "        layercount = len(self.LayerArr)\n",
    "\n",
    "        while (errorrate > errorThresh):\n",
    "\n",
    "            #forward propagation\n",
    "            for i in range(layercount-1):\n",
    "                self.forward(layer1=self.LayerArr[i],layer2=self.LayerArr[i+1])\n",
    "            \n",
    "            #error analysis\n",
    "            errorrate = self.findError()\n",
    "            # print(errorrate)\n",
    "            if(errorrate <errorThresh):\n",
    "                print(\"optimised\")\n",
    "                print(self.LayerArr[-1].neuronvals)\n",
    "                break\n",
    "          \n",
    "\n",
    "            #backward propagation\n",
    "            currentLayeridx = layercount-1  \n",
    "            for i in range(layercount-1):\n",
    "\n",
    "                if self.LayerArr[currentLayeridx].position ==-1:# handling output layer \n",
    "                    self.backPropagate(currentLayer=self.LayerArr[currentLayeridx])\n",
    "                    print(self.LayerArr[currentLayeridx].neuronvals)\n",
    "                    currentLayeridx=currentLayeridx-1\n",
    "                else:\n",
    "                    self.backPropagate(currentLayer=self.LayerArr[currentLayeridx],AfterLayer=self.LayerArr[currentLayeridx+1])\n",
    "                    currentLayeridx=currentLayeridx-1\n",
    "                              \n",
    "    def bind(self,layer=None):\n",
    "        \"\"\"\n",
    "        use this function to add layers into network\n",
    "        \n",
    "        if input layer give coresponding value too\n",
    "        \"\"\"\n",
    "        self.LayerArr.append(layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3.0220934142793494, 3.6935470208219914]\n",
      "[18.507526165254802, 19.720686320850596]\n",
      "[18.26604774476737, 19.46907178774636]\n",
      "[18.033565602234912, 19.226918468928293]\n",
      "[17.809617999659093, 18.99373907460006]\n",
      "[17.593774780828408, 18.76907967394343]\n",
      "[17.38563466789653, 18.55251683889421]\n",
      "[17.184822833699375, 18.343655079273994]\n",
      "[16.990988717365077, 18.142124534991915]\n",
      "[16.803804055078494, 17.947578895582513]\n",
      "[16.622961101534734, 17.759693521227092]\n",
      "[16.44817102075686, 17.57816374272489]\n",
      "[16.279162427645836, 17.4027033207261]\n",
      "[16.115680063946108, 17.23304304698549]\n",
      "[15.957483594305852, 17.06892947250404]\n",
      "[15.804346509835202, 16.910123749248488]\n",
      "[15.656055128059597, 16.756400573716718]\n",
      "[15.512407679461859, 16.607547221987367]\n",
      "[15.3732134719351, 16.46336266708427]\n",
      "[15.238292125452391, 16.323656770525922]\n",
      "[15.107472870118892, 16.18824954083896]\n",
      "[14.9805939015251, 16.056970452609807]\n",
      "[14.857501787980405, 15.929657820347117]\n",
      "[14.73805092478679, 15.806158222040786]\n",
      "[14.622103031224064, 15.68632596784411]\n",
      "[14.509526686369153, 15.570022609782178]\n",
      "[14.40019690027086, 15.457116488811238]\n",
      "[14.293994717354671, 15.34748231592678]\n",
      "[14.190806849245387, 15.241000784349133]\n",
      "[14.090525334473513, 15.137558210109253]\n",
      "[13.99304722277892, 15.037046198618931]\n",
      "[13.898274281945914, 14.939361335042825]\n",
      "[13.806112725300608, 14.844404896497563]\n",
      "[13.716472958177473, 14.752082584289152]\n",
      "[13.629269341819418, 14.662304274566289]\n",
      "[13.544419973316835, 14.574983785916213]\n",
      "[13.461846480317762, 14.490038662563709]\n",
      "[13.381473829355015, 14.407389971953844]\n",
      "[13.303230146738684, 14.326962115607605]\n",
      "[13.227046551054602, 14.248682652236766]\n",
      "[13.152856996392597, 14.172482132192396]\n",
      "[13.080598125503524, 14.098293942400835]\n",
      "[13.010209132152095, 14.026054161012757]\n",
      "[12.941631631993964, 13.955701421055906]\n",
      "[12.874809541361314, 13.887176782441053]\n",
      "[12.809688963391817, 13.820423611724136]\n",
      "[12.746218080981695, 13.75538746907609]\n",
      "[12.684347056085453, 13.69201600195597]\n",
      "[12.62402793492286, 13.630258845023196]\n",
      "[12.56521455868829, 13.570067525861255]\n",
      "[12.507862479389255, 13.511395376118625]\n",
      "[12.45192888046969, 13.45419744770306]\n",
      "[12.397372501899905, 13.398430433693317]\n",
      "[12.344153569439197, 13.344052593657704]\n",
      "[12.292233727799198, 13.2910236830922]\n",
      "[12.241575977456193, 13.239304886712278]\n",
      "[12.192144614879133, 13.188858755351948]\n",
      "[12.14390517595713, 13.139649146241725]\n",
      "[12.096824382425769, 13.091641166453515]\n",
      "[12.050870091105969, 13.044801119315643]\n",
      "[12.006011245782256, 12.999096453615264]\n",
      "[11.962217831559531, 12.954495715418071]\n",
      "[11.919460831548587, 12.910968502347197]\n",
      "[11.877712185740902, 12.868485420174032]\n",
      "[11.836944751942909, 12.827018041583734]\n",
      "[11.797132268648514, 12.786538866987595]\n",
      "[11.758249319737011, 12.747021287262852]\n",
      "[11.720271300890875, 12.708439548308679]\n",
      "[11.683174387634967, 12.67076871731427]\n",
      "[11.64693550490508, 12.633984650641786]\n",
      "[11.611532298059814, 12.59806396323335]\n",
      "[11.57694310525515, 12.562983999456922]\n",
      "[11.543146931106392, 12.5287228053115]\n",
      "[11.510123421566826, 12.495259101917014]\n",
      "[11.477852839956896, 12.462572260219053]\n",
      "[11.446316044081819, 12.43064227684279]\n",
      "[11.415494464379389, 12.399449751034675]\n",
      "[11.385370083043348, 12.368975862634118]\n",
      "[11.355925414070853, 12.339202351020925]\n",
      "[11.327143484185912, 12.310111494987511]\n",
      "[11.299007814593317, 12.281686093488066]\n",
      "[11.271502403520469, 12.253909447219456]\n",
      "[11.244611709506863, 12.22676534099159]\n",
      "[11.218320635403469, 12.200238026847218]\n",
      "[11.192614513046342, 12.174312207893585]\n",
      "[11.167479088570925, 12.148973022810498]\n",
      "[11.142900508335352, 12.124206031001325]\n",
      "[11.118865305422919, 12.09999719835546]\n",
      "[11.095360386695527, 12.076332883592464]\n",
      "[11.072373020371483, 12.053199825159755]\n",
      "[11.04989082410256, 12.03058512865741]\n",
      "[11.027901753526525, 12.008476254764876]\n",
      "optimised\n",
      "[11.006394091272726, 11.986861007646002]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# inputl = Layer(2,1,[2,3])\n",
    "inputl = Layer(neuronCount=2,position=1,neuronvals=[2,3])\n",
    "hidden = Layer(neuronCount=2,position=2,neuronvals=[])\n",
    "# hidden2 = Layer(neuronCount=2,position=3,neuronvals=[]) \n",
    "output = Layer(neuronCount=2,position=-1,neuronvals=[])\n",
    "\n",
    "Ann = Ann()\n",
    "Ann.bind(layer=inputl)\n",
    "Ann.bind(layer=hidden)\n",
    "# Ann.bind(layer=hidden2)\n",
    "Ann.bind(layer=output)\n",
    "Ann.compile()\n",
    "Ann.Train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
